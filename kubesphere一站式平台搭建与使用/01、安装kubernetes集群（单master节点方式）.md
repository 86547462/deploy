# 01ã€å®‰è£…kubernetesé›†ç¾¤ï¼ˆå•masterèŠ‚ç‚¹æ–¹å¼ï¼‰

# è§†é¢‘åœ°å€
[bilibili](https://player.bilibili.com/player.html?bvid=BV1f54y1y7Vn&p=1&page=1)

# ä¸€ã€ç¯å¢ƒå‡†å¤‡
## 1ã€ç³»ç»Ÿè¦æ±‚
æŒ‰é‡ä»˜è´¹é˜¿é‡Œäº‘ä¸»æœºä¸‰å°

è¦æ±‚ï¼š**centos7.6~7.8**ï¼›ä»¥ä¸‹ä¸º [https://kuboard.cn/install/install-k8s.html#%E6%A3%80%E6%9F%A5-centos-hostname](https://kuboard.cn/install/install-k8s.html#%E6%A3%80%E6%9F%A5-centos-hostname) ç½‘ç«™çš„æ£€éªŒç»“æœã€‚

| CentOS ç‰ˆæœ¬ | æœ¬æ–‡æ¡£æ˜¯å¦å…¼å®¹ | å¤‡æ³¨ |
| :--- | :--- | :--- |
| 7.8 | ğŸ˜„ | å·²éªŒè¯ |
| 7.7 | ğŸ˜„ | å·²éªŒè¯ |
| 7.6 | ğŸ˜„ | å·²éªŒè¯ |
| 7.5 | ğŸ˜ | å·²è¯å®ä¼šå‡ºç° kubelet æ— æ³•å¯åŠ¨çš„é—®é¢˜ |
| 7.4 | ğŸ˜ | å·²è¯å®ä¼šå‡ºç° kubelet æ— æ³•å¯åŠ¨çš„é—®é¢˜ |
| 7.3 | ğŸ˜ | å·²è¯å®ä¼šå‡ºç° kubelet æ— æ³•å¯åŠ¨çš„é—®é¢˜ |
| 7.2 | ğŸ˜ | å·²è¯å®ä¼šå‡ºç° kubelet æ— æ³•å¯åŠ¨çš„é—®é¢˜ |




## 2ã€å‰ç½®æ­¥éª¤ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰
+ centos ç‰ˆæœ¬ä¸º 7.6 æˆ– 7.7ã€CPU å†…æ ¸æ•°é‡å¤§äºç­‰äº 2ï¼Œä¸”å†…å­˜å¤§äºç­‰äº 4G
+ hostname ä¸æ˜¯ localhostï¼Œä¸”ä¸åŒ…å«ä¸‹åˆ’çº¿ã€å°æ•°ç‚¹ã€å¤§å†™å­—æ¯
+ ä»»æ„èŠ‚ç‚¹éƒ½æœ‰å›ºå®šçš„å†…ç½‘ IP åœ°å€(é›†ç¾¤æœºå™¨ç»Ÿä¸€å†…ç½‘)
+ ä»»æ„èŠ‚ç‚¹ä¸Š IP åœ°å€ å¯äº’é€šï¼ˆæ— éœ€ NAT æ˜ å°„å³å¯ç›¸äº’è®¿é—®ï¼‰ï¼Œä¸”æ²¡æœ‰é˜²ç«å¢™ã€å®‰å…¨ç»„éš”ç¦»
+ ä»»æ„èŠ‚ç‚¹ä¸ä¼šç›´æ¥ä½¿ç”¨ docker run æˆ– docker-compose è¿è¡Œå®¹å™¨ã€‚Pod

```shell
#å…³é—­é˜²ç«å¢™ï¼š æˆ–è€…é˜¿é‡Œäº‘å¼€é€šå®‰å…¨ç»„ç«¯å£è®¿é—®
systemctl stop firewalld
systemctl disable firewalld

#å…³é—­ selinuxï¼š 
sed -i 's/enforcing/disabled/' /etc/selinux/config
setenforce 0

#å…³é—­ swapï¼š
swapoff -a  #ä¸´æ—¶ 
sed -ri 's/.*swap.*/#&/' /etc/fstab  #æ°¸ä¹…

#å°†æ¡¥æ¥çš„ IPv4 æµé‡ä¼ é€’åˆ° iptables çš„é“¾ï¼š
# ä¿®æ”¹ /etc/sysctl.conf
# å¦‚æœæœ‰é…ç½®ï¼Œåˆ™ä¿®æ”¹
sed -i "s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g"  /etc/sysctl.conf
sed -i "s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g"  /etc/sysctl.conf
sed -i "s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g"  /etc/sysctl.conf
sed -i "s#^net.ipv6.conf.all.disable_ipv6.*#net.ipv6.conf.all.disable_ipv6=1#g"  /etc/sysctl.conf
sed -i "s#^net.ipv6.conf.default.disable_ipv6.*#net.ipv6.conf.default.disable_ipv6=1#g"  /etc/sysctl.conf
sed -i "s#^net.ipv6.conf.lo.disable_ipv6.*#net.ipv6.conf.lo.disable_ipv6=1#g"  /etc/sysctl.conf
sed -i "s#^net.ipv6.conf.all.forwarding.*#net.ipv6.conf.all.forwarding=1#g"  /etc/sysctl.conf
# å¯èƒ½æ²¡æœ‰ï¼Œè¿½åŠ 
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf
echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf
echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
echo "net.ipv6.conf.lo.disable_ipv6 = 1" >> /etc/sysctl.conf
echo "net.ipv6.conf.all.forwarding = 1"  >> /etc/sysctl.conf
# æ‰§è¡Œå‘½ä»¤ä»¥åº”ç”¨
sysctl -p
```



# äºŒã€å®‰è£…Dockerç¯å¢ƒï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰
```shell
#1ã€å®‰è£…docker
##1.1ã€å¸è½½æ—§ç‰ˆæœ¬
sudo yum remove docker \
	docker-client \
	docker-client-latest \
	docker-common \
	docker-latest \
	docker-latest-logrotate \
	docker-logrotate \
	docker-engine
##1.2ã€å®‰è£…åŸºç¡€ä¾èµ–
yum install -y yum-utils \
device-mapper-persistent-data \
lvm2

##1.3ã€é…ç½®docker yumæº
sudo yum-config-manager \
--add-repo \
http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

##1.4ã€å®‰è£…å¹¶å¯åŠ¨ docker
yum install -y docker-ce-19.03.8 docker-ce-cli-19.03.8 containerd.io
systemctl enable docker
systemctl start docker

##1.5ã€é…ç½®dockeråŠ é€Ÿ
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://t1gbabbr.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```



# ä¸‰ã€å®‰è£…k8sç¯å¢ƒ
## 1ã€å®‰è£…k8sã€kubeletã€kubeadmã€kubectlï¼ˆæ‰€æœ‰èŠ‚ç‚¹ï¼‰
```shell
# é…ç½®K8Sçš„yumæº
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# å¸è½½æ—§ç‰ˆæœ¬
yum remove -y kubelet kubeadm kubectl

# å®‰è£…kubeletã€kubeadmã€kubectl
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3

#å¼€æœºå¯åŠ¨å’Œé‡å¯kubelet
systemctl enable kubelet && systemctl start kubelet
##æ³¨æ„ï¼Œå¦‚æœæ­¤æ—¶æŸ¥çœ‹kubeletçš„çŠ¶æ€ï¼Œä»–ä¼šæ— é™é‡å¯ï¼Œç­‰å¾…æ¥æ”¶é›†ç¾¤å‘½ä»¤ï¼Œå’Œåˆå§‹åŒ–ã€‚è¿™ä¸ªæ˜¯æ­£å¸¸çš„ã€‚
```

## 2ã€åˆå§‹åŒ–masterèŠ‚ç‚¹ï¼ˆmasterèŠ‚ç‚¹ï¼‰
```shell
#1ã€ä¸‹è½½masterèŠ‚ç‚¹éœ€è¦çš„é•œåƒã€é€‰åšã€‘
#åˆ›å»ºä¸€ä¸ª.shæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼Œ
#!/bin/bash
images=(
	kube-apiserver:v1.17.3
    kube-proxy:v1.17.3
	kube-controller-manager:v1.17.3
	kube-scheduler:v1.17.3
	coredns:1.6.5
	etcd:3.4.3-0
    pause:3.1
)
for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done


#2ã€åˆå§‹åŒ–masterèŠ‚ç‚¹
kubeadm init \
--apiserver-advertise-address=172.26.165.243 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
--kubernetes-version v1.17.3 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=192.168.0.0/16


#serviceç½‘ç»œå’Œpodç½‘ç»œï¼›docker service create 
#docker container --> ip brigde
#Pod ---> ip åœ°å€ï¼Œæ•´ä¸ªé›†ç¾¤ Pod æ˜¯å¯ä»¥äº’é€šã€‚255*255
#service ---> 

#3ã€é…ç½® kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#4ã€æå‰ä¿å­˜ä»¤ç‰Œ
kubeadm join 172.26.165.243:6443 --token afb6st.b7jz45ze7zpg65ii \
    --discovery-token-ca-cert-hash sha256:e5e5854508dafd04f0e9cf1f502b5165e25ff3017afd23cade0fe6acb5bc14ab

#5ã€éƒ¨ç½²ç½‘ç»œæ’ä»¶
#ä¸Šä¼ ç½‘ç»œæ’ä»¶ï¼Œå¹¶éƒ¨ç½²
#kubectl apply -f calico-3.13.1.yaml
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

#ç½‘ç»œå¥½çš„æ—¶å€™ï¼Œå°±æ²¡æœ‰ä¸‹é¢çš„æ“ä½œäº†
calicoï¼š
image: calico/cni:v3.14.0
image: calico/cni:v3.14.0
image: calico/pod2daemon-flexvol:v3.14.0
image: calico/node:v3.14.0
image: calico/kube-controllers:v3.14.0




#6ã€æŸ¥çœ‹çŠ¶æ€ï¼Œç­‰å¾…å°±ç»ª
watch kubectl get pod -n kube-system -o wide
```



## 3ã€workeråŠ å…¥é›†ç¾¤
```shell
#1ã€ä½¿ç”¨åˆšæ‰masteræ‰“å°çš„ä»¤ç‰Œå‘½ä»¤åŠ å…¥
kubeadm join 172.26.248.150:6443 --token ktnvuj.tgldo613ejg5a3x4 \
    --discovery-token-ca-cert-hash sha256:f66c496cf7eb8aa06e1a7cdb9b6be5b013c613cdcf5d1bbd88a6ea19a2b454ec
#2ã€å¦‚æœè¶…è¿‡2å°æ—¶å¿˜è®°äº†ä»¤ç‰Œï¼Œå¯ä»¥è¿™æ ·åš
kubeadm token create --print-join-command #æ‰“å°æ–°ä»¤ç‰Œ
kubeadm token create --ttl 0 --print-join-command #åˆ›å»ºä¸ªæ°¸ä¸è¿‡æœŸçš„ä»¤ç‰Œ
```

## 4ã€æ­å»ºNFSä½œä¸ºé»˜è®¤sc
### 4.1ã€é…ç½®NFSæœåŠ¡å™¨
```shell
yum install -y nfs-utils
#æ‰§è¡Œå‘½ä»¤ vi /etc/exportsï¼Œåˆ›å»º exports æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š
echo "/nfs/data/ *(insecure,rw,sync,no_root_squash)" > /etc/exports
#/nfs/data  172.26.248.0/20(rw,no_root_squash)
#æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨ nfs æœåŠ¡
# åˆ›å»ºå…±äº«ç›®å½•
mkdir -p /nfs/data
systemctl enable rpcbind
systemctl enable nfs-server
systemctl start rpcbind
systemctl start nfs-server
exportfs -r
#æ£€æŸ¥é…ç½®æ˜¯å¦ç”Ÿæ•ˆ
exportfs
# è¾“å‡ºç»“æœå¦‚ä¸‹æ‰€ç¤º
/nfs/data /nfs/data
```

```yaml
#æµ‹è¯•Podç›´æ¥æŒ‚è½½NFSäº†
apiVersion: v1
kind: Pod
metadata:
  name: vol-nfs
  namespace: default
spec:
  volumes:
  - name: html
    nfs:
      path: /nfs/data   #1000G
      server: è‡ªå·±çš„nfsæœåŠ¡å™¨åœ°å€
  containers:
  - name: myapp
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html/
```

### 4.2ã€æ­å»ºNFS-Client
```shell
#æœåŠ¡å™¨ç«¯é˜²ç«å¢™å¼€æ”¾111ã€662ã€875ã€892ã€2049çš„ tcp / udp å…è®¸ï¼Œå¦åˆ™è¿œç«¯å®¢æˆ·æ— æ³•è¿æ¥ã€‚
#å®‰è£…å®¢æˆ·ç«¯å·¥å…·
yum install -y nfs-utils


#æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ nfs æœåŠ¡å™¨ç«¯æ˜¯å¦æœ‰è®¾ç½®å…±äº«ç›®å½•
# showmount -e $(nfsæœåŠ¡å™¨çš„IP)
showmount -e 172.26.165.243
# è¾“å‡ºç»“æœå¦‚ä¸‹æ‰€ç¤º
Export list for 172.26.165.243
/nfs/data *

#æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æŒ‚è½½ nfs æœåŠ¡å™¨ä¸Šçš„å…±äº«ç›®å½•åˆ°æœ¬æœºè·¯å¾„ /root/nfsmount
mkdir /root/nfsmount
# mount -t nfs $(nfsæœåŠ¡å™¨çš„IP):/root/nfs_root /root/nfsmount
#é«˜å¯ç”¨å¤‡ä»½çš„æ–¹å¼
mount -t nfs 172.26.165.243:/nfs/data /root/nfsmount
# å†™å…¥ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
echo "hello nfs server" > /root/nfsmount/test.txt

#åœ¨ nfs æœåŠ¡å™¨ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒéªŒè¯æ–‡ä»¶å†™å…¥æˆåŠŸ
cat /data/volumes/test.txt
```





### 4.3ã€è®¾ç½®åŠ¨æ€ä¾›åº”
![1600661166411-64bfb5e4-c8cf-4932-979b-bb962f94ec8b.png](./img/IJVJV6Jv3PDop3BD/1600661166411-64bfb5e4-c8cf-4932-979b-bb962f94ec8b-967713.png)



#### 4.3.1ã€åˆ›å»ºprovisionerï¼ˆNFSç¯å¢ƒå‰é¢å·²ç»æ­å¥½ï¼‰
| å­—æ®µåç§° | å¡«å…¥å†…å®¹ | å¤‡æ³¨ |
| :--- | :--- | :--- |
| åç§° | nfs-storage | è‡ªå®šä¹‰å­˜å‚¨ç±»åç§° |
| NFS Server | 172.26.165.243 | NFSæœåŠ¡çš„IPåœ°å€ |
| NFS Path | /nfs/data | NFSæœåŠ¡æ‰€å…±äº«çš„è·¯å¾„ |


<font style="color:#333333;"></font>

```yaml
# å…ˆåˆ›å»ºæˆæƒ
# vi nfs-rbac.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
   name: nfs-provisioner-runner
rules:
   -  apiGroups: [""]
      resources: ["persistentvolumes"]
      verbs: ["get", "list", "watch", "create", "delete"]
   -  apiGroups: [""]
      resources: ["persistentvolumeclaims"]
      verbs: ["get", "list", "watch", "update"]
   -  apiGroups: ["storage.k8s.io"]
      resources: ["storageclasses"]
      verbs: ["get", "list", "watch"]
   -  apiGroups: [""]
      resources: ["events"]
      verbs: ["watch", "create", "update", "patch"]
   -  apiGroups: [""]
      resources: ["services", "endpoints"]
      verbs: ["get","create","list", "watch","update"]
   -  apiGroups: ["extensions"]
      resources: ["podsecuritypolicies"]
      resourceNames: ["nfs-provisioner"]
      verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
#vi nfs-deployment.yamlï¼›åˆ›å»ºnfs-clientçš„æˆæƒ
kind: Deployment
apiVersion: apps/v1
metadata:
   name: nfs-client-provisioner
spec:
   replicas: 1
   strategy:
     type: Recreate
   selector:
     matchLabels:
        app: nfs-client-provisioner
   template:
      metadata:
         labels:
            app: nfs-client-provisioner
      spec:
         serviceAccount: nfs-provisioner
         containers:
            -  name: nfs-client-provisioner
               image: lizhenliang/nfs-client-provisioner
               volumeMounts:
                 -  name: nfs-client-root
                    mountPath:  /persistentvolumes
               env:
                 -  name: PROVISIONER_NAME #ä¾›åº”è€…çš„åå­—
                    value: storage.pri/nfs #åå­—è™½ç„¶å¯ä»¥éšä¾¿èµ·ï¼Œä»¥åå¼•ç”¨è¦ä¸€è‡´
                 -  name: NFS_SERVER
                    value: 172.26.165.243
                 -  name: NFS_PATH
                    value: /nfs/data
         volumes:
           - name: nfs-client-root
             nfs:
               server: 172.26.165.243
               path: /nfs/data
##è¿™ä¸ªé•œåƒä¸­volumeçš„mountPathé»˜è®¤ä¸º/persistentvolumesï¼Œä¸èƒ½ä¿®æ”¹ï¼Œå¦åˆ™è¿è¡Œæ—¶ä¼šæŠ¥é”™
```

```yaml
#åˆ›å»ºstorageclass
# vi storageclass-nfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storage-nfs
provisioner: storage.pri/nfs
reclaimPolicy: Delete

#æ‰©å±•"reclaim policy"æœ‰ä¸‰ç§æ–¹å¼ï¼šRetainã€Recycleã€Deletedã€‚
Retain
#ä¿æŠ¤è¢«PVCé‡Šæ”¾çš„PVåŠå…¶ä¸Šæ•°æ®ï¼Œå¹¶å°†PVçŠ¶æ€æ”¹æˆ"released"ï¼Œä¸å°†è¢«å…¶å®ƒPVCç»‘å®šã€‚é›†ç¾¤ç®¡ç†å‘˜æ‰‹åŠ¨é€šè¿‡å¦‚ä¸‹æ­¥éª¤é‡Šæ”¾å­˜å‚¨èµ„æºï¼š
æ‰‹åŠ¨åˆ é™¤PVï¼Œä½†ä¸å…¶ç›¸å…³çš„åç«¯å­˜å‚¨èµ„æºå¦‚(AWS EBS, GCE PD, Azure Disk, or Cinder volume)ä»ç„¶å­˜åœ¨ã€‚
æ‰‹åŠ¨æ¸…ç©ºåç«¯å­˜å‚¨volumeä¸Šçš„æ•°æ®ã€‚
æ‰‹åŠ¨åˆ é™¤åç«¯å­˜å‚¨volumeï¼Œæˆ–è€…é‡å¤ä½¿ç”¨åç«¯volumeï¼Œä¸ºå…¶åˆ›å»ºæ–°çš„PVã€‚

Delete
åˆ é™¤è¢«PVCé‡Šæ”¾çš„PVåŠå…¶åç«¯å­˜å‚¨volumeã€‚å¯¹äºåŠ¨æ€PVå…¶"reclaim policy"ç»§æ‰¿è‡ªå…¶"storage class"ï¼Œ
é»˜è®¤æ˜¯Deleteã€‚é›†ç¾¤ç®¡ç†å‘˜è´Ÿè´£å°†"storage class"çš„"reclaim policy"è®¾ç½®æˆç”¨æˆ·æœŸæœ›çš„å½¢å¼ï¼Œå¦åˆ™éœ€è¦ç”¨
æˆ·æ‰‹åŠ¨ä¸ºåˆ›å»ºåçš„åŠ¨æ€PVç¼–è¾‘"reclaim policy"

Recycle
ä¿ç•™PVï¼Œä½†æ¸…ç©ºå…¶ä¸Šæ•°æ®ï¼Œå·²åºŸå¼ƒ

```

#### 4.3.2ã€åˆ›å»ºå­˜å‚¨ç±»
```yaml
#åˆ›å»ºstorageclass
# vi storageclass-nfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: storage-nfs
provisioner: storage.pri/nfs
reclaimPolicy: Delete

```

Â "reclaim policy"æœ‰ä¸‰ç§æ–¹å¼ï¼šRetainã€Recycleã€Deletedã€‚

+ Retain
    - ä¿æŠ¤è¢«PVCé‡Šæ”¾çš„PVåŠå…¶ä¸Šæ•°æ®ï¼Œå¹¶å°†PVçŠ¶æ€æ”¹æˆ"released"ï¼Œä¸å°†è¢«å…¶å®ƒPVCç»‘å®šã€‚é›†ç¾¤ç®¡ç†å‘˜æ‰‹åŠ¨é€šè¿‡å¦‚ä¸‹æ­¥éª¤é‡Šæ”¾å­˜å‚¨èµ„æº
        * æ‰‹åŠ¨åˆ é™¤PVï¼Œä½†ä¸å…¶ç›¸å…³çš„åç«¯å­˜å‚¨èµ„æºå¦‚(AWS EBS, GCE PD, Azure Disk, or Cinder volume)ä»ç„¶å­˜åœ¨ã€‚
        * æ‰‹åŠ¨æ¸…ç©ºåç«¯å­˜å‚¨volumeä¸Šçš„æ•°æ®ã€‚
        * æ‰‹åŠ¨åˆ é™¤åç«¯å­˜å‚¨volumeï¼Œæˆ–è€…é‡å¤ä½¿ç”¨åç«¯volumeï¼Œä¸ºå…¶åˆ›å»ºæ–°çš„PVã€‚
+ Delete
    - åˆ é™¤è¢«PVCé‡Šæ”¾çš„PVåŠå…¶åç«¯å­˜å‚¨volumeã€‚å¯¹äºåŠ¨æ€PVå…¶"reclaim policy"ç»§æ‰¿è‡ªå…¶"storage class"ï¼Œ
    - é»˜è®¤æ˜¯Deleteã€‚é›†ç¾¤ç®¡ç†å‘˜è´Ÿè´£å°†"storage class"çš„"reclaim policy"è®¾ç½®æˆç”¨æˆ·æœŸæœ›çš„å½¢å¼ï¼Œå¦åˆ™éœ€è¦ç”¨æˆ·æ‰‹åŠ¨ä¸ºåˆ›å»ºåçš„åŠ¨æ€PVç¼–è¾‘"reclaim policy"
+ Recycle
    - ä¿ç•™PVï¼Œä½†æ¸…ç©ºå…¶ä¸Šæ•°æ®ï¼Œå·²åºŸå¼ƒ





#### 4.3.3ã€æ”¹å˜é»˜è®¤sc
```shell
##æ”¹å˜ç³»ç»Ÿé»˜è®¤sc
https://kubernetes.io/zh/docs/tasks/administer-cluster/change-default-storage-class/#%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e6%94%b9%e5%8f%98%e9%bb%98%e8%ae%a4-storage-class

kubectl patch storageclass storage-nfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

### 4.4ã€éªŒè¯nfsåŠ¨æ€ä¾›åº”
#### 4.4.1ã€åˆ›å»ºpvc
```yaml
#vi  pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-claim-01
 # annotations:
 #   volume.beta.kubernetes.io/storage-class: "storage-nfs"
spec:
  storageClassName: storage-nfs  #è¿™ä¸ªclassä¸€å®šæ³¨æ„è¦å’Œscçš„åå­—ä¸€æ ·
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
```



#### 4.4.2ã€ä½¿ç”¨pvc
```yaml
#vi testpod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: test-pod
spec:
  containers:
  - name: test-pod
    image: busybox
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: pvc-claim-01
```

## 5ã€å®‰è£…metrics-server
```yaml
#1ã€å…ˆå®‰è£…metrics-server(yamlå¦‚ä¸‹ï¼Œå·²ç»æ”¹å¥½äº†é•œåƒå’Œé…ç½®ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨)ï¼Œè¿™æ ·å°±èƒ½ç›‘æ§åˆ°podã€‚nodeçš„èµ„æºæƒ…å†µï¼ˆé»˜è®¤åªæœ‰cpuã€memoryçš„èµ„æºå®¡è®¡ä¿¡æ¯å“Ÿï¼Œæ›´ä¸“ä¸šçš„æˆ‘ä»¬åé¢å¯¹æ¥ Prometheusï¼‰
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:aggregated-metrics-reader
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
rules:
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
---
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  service:
    name: metrics-server
    namespace: kube-system
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    k8s-app: metrics-server
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      name: metrics-server
      labels:
        k8s-app: metrics-server
    spec:
      serviceAccountName: metrics-server
      volumes:
      # mount in tmp so we can safely use from-scratch images and/or read-only containers
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.6
        imagePullPolicy: IfNotPresent
        args:
          - --cert-dir=/tmp
          - --secure-port=4443
          - --kubelet-insecure-tls
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        ports:
        - name: main-port
          containerPort: 4443
          protocol: TCP
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
      nodeSelector:
        kubernetes.io/os: linux
        kubernetes.io/arch: "amd64"
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    kubernetes.io/name: "Metrics-server"
    kubernetes.io/cluster-service: "true"
spec:
  selector:
    k8s-app: metrics-server
  ports:
  - port: 443
    protocol: TCP
    targetPort: main-port
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
```

## 


> æ›´æ–°: 2024-07-23 10:31:06  
> åŸæ–‡: <https://www.yuque.com/leifengyang/kubesphere/grw8se>